{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c1f2f5",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "- RMSE : 26185.4795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install eli5==0.13.0\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "!apt-get install -y fonts-nanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7eeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # ttf íŒŒì¼ì´ ì €ì¥ë˜ì–´ ìˆëŠ” ê²½ë¡œ\n",
    "    name='NanumBarunGothic')                        # ì´ í°íŠ¸ì˜ ì›í•˜ëŠ” ì´ë¦„ ì„¤ì •\n",
    "fm.fontManager.ttflist.insert(0, fe)              # Matplotlibì— í°íŠ¸ ì¶”ê°€\n",
    "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # í°íŠ¸ ì„¤ì •\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "import seaborn as sns\n",
    "\n",
    "# utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—´ ë‹¤ ë³´ì´ê²Œ ì„¤ì •\n",
    "pd.set_option('display.max_columns', None)\n",
    "# í–‰ ë‹¤ ë³´ì´ê²Œ ì„¤ì •\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e82c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "train_path = '/root/AI_STAGE/upstageailab-ml-competition-ml-2/1.Data/train.csv'\n",
    "test_path  = '/root/AI_STAGE/upstageailab-ml-competition-ml-2/1.Data/test.csv'\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d43d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test êµ¬ë¶„ì„ ìœ„í•œ ì¹¼ëŸ¼(data)\n",
    "# trainê³¼ testë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°ë¡œ ë§Œë“¤ê¸°\n",
    "\n",
    "train['data'] = 0\n",
    "test['data'] = 1\n",
    "concat = pd.concat([train, test])\n",
    "\n",
    "print(concat.shape)\n",
    "print(concat['data'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—´ ì „ì²´ë¥¼ ë„£ê³  ìŠ¤ìº”í•˜ê¸°\n",
    "train.info()\n",
    "for col in concat.columns:\n",
    "    nunique = concat[col].nunique(dropna=False)\n",
    "    missing_ratio = concat[col].isna().mean()\n",
    "    missing_count = concat[col].isnull().sum()\n",
    "    col_type = concat.dtypes[col]\n",
    "    print(f\"ğŸ“Œ {col:30} | ë°ì´í„°íƒ€ì…: {col_type} | ê³ ìœ ê°’: {nunique:6} | ê²°ì¸¡ê°œìˆ˜: {missing_count} | ê²°ì¸¡ë¥ : {missing_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ëŠ” ì•„ë‹Œë° ì˜ë¯¸ ì—†ëŠ” í˜•ì‹ì  ê°’ ì°¾ê¸°\n",
    "\n",
    "def detect_fake_nulls(df, suspect_values=['-', ' ', '', '.', 'ì—†ìŒ', 'nan']):\n",
    "    result = {}\n",
    "    for col in df.columns:\n",
    "        if concat[col].dtype == 'object':\n",
    "            val_counts = concat[col].value_counts(dropna=False)\n",
    "            found = val_counts[val_counts.index.isin(suspect_values)]\n",
    "            if not found.empty:\n",
    "                result[col] = found\n",
    "    return result\n",
    "\n",
    "fake_nulls = detect_fake_nulls(concat)\n",
    "for col, vals in fake_nulls.items():\n",
    "    print(f\"ğŸ” {col} ì»¬ëŸ¼ì—ì„œ ì˜ë¯¸ ì—†ëŠ” ê°’ ë°œê²¬:\")\n",
    "    print(vals)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071421af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ„ ì²˜ëŸ¼ ì•„ë¬´ ì˜ë¯¸ë„ ê°–ì§€ ì•ŠëŠ” ì¹¼ëŸ¼ì€ ê²°ì¸¡ì¹˜ì™€ ê°™ì€ ì—­í• ì„ í•˜ë¯€ë¡œ, np.nanìœ¼ë¡œ ì±„ì›Œ ê²°ì¸¡ì¹˜ë¡œ ì¸ì‹ë˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
    "concat['ë„ë¡œëª…'] = concat['ë„ë¡œëª…'].replace(' ', np.nan)\n",
    "concat['ë“±ê¸°ì‹ ì²­ì¼ì'] = concat['ë“±ê¸°ì‹ ì²­ì¼ì'].replace(' ', np.nan)\n",
    "concat['ê±°ë˜ìœ í˜•'] = concat['ê±°ë˜ìœ í˜•'].replace('-', np.nan)\n",
    "concat['ì¤‘ê°œì‚¬ì†Œì¬ì§€'] = concat['ì¤‘ê°œì‚¬ì†Œì¬ì§€'].replace('-', np.nan)\n",
    "concat['k-ì‹œí–‰ì‚¬'] = concat['k-ì‹œí–‰ì‚¬'].replace('.', np.nan)\n",
    "concat['k-ì‹œí–‰ì‚¬'] = concat['k-ì‹œí–‰ì‚¬'].replace('-', np.nan)\n",
    "concat['k-í™ˆí˜ì´ì§€'] = concat['k-í™ˆí˜ì´ì§€'].replace('ì—†ìŒ', np.nan)\n",
    "concat['k-í™ˆí˜ì´ì§€'] = concat['k-í™ˆí˜ì´ì§€'].replace('.', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af291c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ìˆ˜ë³„ ê²°ì¸¡ì¹˜ì˜ ë¹„ìœ¨ì„ plotìœ¼ë¡œ ê·¸ë ¤ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "fig = plt.figure(figsize=(13, 2))\n",
    "missing = concat.isnull().sum() / concat.shape[0]\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot.bar(color='orange')\n",
    "plt.title('ë³€ìˆ˜ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d064d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ 90ë§Œê°œ ì´ìƒì¸ ê°’ê³¼ ì´í•˜ì§€ë§Œ í•„ìš”ì—†ëŠ” ê²ƒ ì œì™¸\n",
    "# í•„ìš”ì—†ì–´ ë³´ì´ëŠ” ê²ƒ : k-ì „í™”ë²ˆí˜¸, k-íŒ©ìŠ¤ë²ˆí˜¸, ì‚¬ìš©í—ˆê°€ì—¬ë¶€, ê´€ë¦¬ë¹„ ì—…ë¡œë“œ, k-ìˆ˜ì •ì¼ì\n",
    "\n",
    "valid_cols = concat.columns[concat.isnull().sum() <= 900000]\n",
    "exclude_cols = ['k-ì „í™”ë²ˆí˜¸', 'k-íŒ©ìŠ¤ë²ˆí˜¸', 'ì‚¬ìš©í—ˆê°€ì—¬ë¶€', 'ê´€ë¦¬ë¹„ ì—…ë¡œë“œ', 'k-ìˆ˜ì •ì¼ì']\n",
    "\n",
    "select = [col for col in valid_cols if col not in exclude_cols]\n",
    "concat_select = concat[select]\n",
    "\n",
    "concat.shape, concat_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_summary(df, columns):\n",
    "    result = pd.DataFrame({\n",
    "        'ê²°ì¸¡ ê°œìˆ˜': df[columns].isnull().sum(),\n",
    "        'ê²°ì¸¡ ë¹„ìœ¨(%)': df[columns].isnull().mean() * 100\n",
    "    })\n",
    "    return result[result['ê²°ì¸¡ ê°œìˆ˜'] > 0].sort_values('ê²°ì¸¡ ë¹„ìœ¨(%)', ascending=False)\n",
    "\n",
    "print(\"ğŸ“Š ì—°ì†í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ìš”ì•½\")\n",
    "display(null_summary(concat_select, con_columns))\n",
    "\n",
    "print(\"ğŸ“Š ë²”ì£¼í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ìš”ì•½\")\n",
    "display(null_summary(concat_select, cat_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3990fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ íƒì§€ ë° ì²˜ë¦¬ ì „ ì—°ì†í˜• ë³€ìˆ˜ ìƒê´€ê´€ê³„ ë³´ê¸°\n",
    "\n",
    "# 1. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ì„ íƒ\n",
    "numeric_cols = concat_select.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# 2. ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "corr = concat_select[numeric_cols].corr()\n",
    "\n",
    "# 3. ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "plt.title(\"ğŸ“Š ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a642e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ìƒê´€ê´€ê³„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ feature ìŒ íƒì§€ ë° ì‚­ì œ í›„ë³´ ì¶”ì²œ\n",
    "\n",
    "# ì—°ì†í˜• ë³€ìˆ˜ë§Œ ì¶”ì¶œ\n",
    "numeric_cols = con_columns  # â† ë„ˆê°€ ë‚˜ëˆ ë‘” ì—°ì†í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# ìƒê´€ê´€ê³„ í–‰ë ¬ (ì ˆëŒ“ê°’ ê¸°ì¤€)\n",
    "corr_matrix = concat_select[numeric_cols].corr().abs()\n",
    "\n",
    "# ìƒì‚¼ê° í–‰ë ¬ë¡œ ì¤‘ë³µ ì œê±°\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# ìƒê´€ê³„ìˆ˜ 0.9 ì´ˆê³¼ì¸ ë³€ìˆ˜ìŒ ì¶”ì¶œ\n",
    "high_corr_pairs = [(col, row, upper.loc[row, col])\n",
    "                   for col in upper.columns\n",
    "                   for row in upper.index\n",
    "                   if pd.notnull(upper.loc[row, col]) and upper.loc[row, col] > 0.7]\n",
    "\n",
    "# ì¶œë ¥\n",
    "for col1, col2, score in sorted(high_corr_pairs, key=lambda x: -x[2]):\n",
    "    print(f\"ğŸ” {col1} â†” {col2} : ìƒê´€ê³„ìˆ˜ = {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598cadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ìš©ì„¸ëŒ€ë³„ë©´ì  PCA ì§„í–‰ ë° ìƒê´€ê´€ê³„ ë†’ì€ ë³€ìˆ˜ ì œê±° í›„ ë°ì´í„° ì‚´í´ë³´ê¸°\n",
    "\n",
    "concat_select.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ecc9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¨¼ì €, ì—°ì†í˜• ë³€ìˆ˜ì™€ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìœ„ infoì— ë”°ë¼ ë¶„ë¦¬í•´ì£¼ê² ìŠµë‹ˆë‹¤.\n",
    "# ìˆ«ìí˜• ë¶„ë¦¬ pd.api.types.is_numeric_dtype\n",
    "con_columns2 = []\n",
    "cat_columns2 = []\n",
    "\n",
    "for column in concat_select.columns:\n",
    "    if pd.api.types.is_numeric_dtype(concat_select[column]):\n",
    "        con_columns2.append(column)\n",
    "    else:\n",
    "        cat_columns2.append(column)\n",
    "\n",
    "print(\"ì—°ì†í˜• ë³€ìˆ˜:\", con_columns2)\n",
    "print(\"ë²”ì£¼í˜• ë³€ìˆ˜:\", cat_columns2)\n",
    "\n",
    "def null_summary(df, columns):\n",
    "    result = pd.DataFrame({\n",
    "        'ê²°ì¸¡ ê°œìˆ˜': df[columns].isnull().sum(),\n",
    "        'ê²°ì¸¡ ë¹„ìœ¨(%)': df[columns].isnull().mean() * 100\n",
    "    })\n",
    "    return result[result['ê²°ì¸¡ ê°œìˆ˜'] > 0].sort_values('ê²°ì¸¡ ë¹„ìœ¨(%)', ascending=False)\n",
    "\n",
    "print(\"ğŸ“Š ì—°ì†í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ìš”ì•½\")\n",
    "display(null_summary(concat_select, con_columns2))\n",
    "\n",
    "print(\"ğŸ“Š ë²”ì£¼í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ìš”ì•½\")\n",
    "display(null_summary(concat_select, cat_columns2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9663df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²”ì£¼í˜• featureë“¤ ê´€ê³„ ë³´ê¸°\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# CramÃ©r's V ê³„ì‚° í•¨ìˆ˜\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2 / min(k - 1, r - 1))\n",
    "\n",
    "# ë²”ì£¼í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "cat_cols = cat_columns2  # ì´ë¯¸ ë‚˜ëˆˆ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© ë§¤íŠ¸ë¦­ìŠ¤\n",
    "cramer_matrix = pd.DataFrame(index=cat_cols, columns=cat_cols)\n",
    "\n",
    "for col1 in cat_cols:\n",
    "    for col2 in cat_cols:\n",
    "        if col1 == col2:\n",
    "            cramer_matrix.loc[col1, col2] = 1.0\n",
    "        else:\n",
    "            try:\n",
    "                cramer_matrix.loc[col1, col2] = cramers_v(concat_select[col1], concat_select[col2])\n",
    "            except:\n",
    "                cramer_matrix.loc[col1, col2] = np.nan\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cramer_matrix.astype(float), cmap='coolwarm', annot=False)\n",
    "plt.title(\"ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ CramÃ©r's V (ìƒê´€ê´€ê³„ ìœ ì‚¬ë„)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163aa28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ì£¼í˜• ì¤‘ uniqueê°’ì´ ë‚®ì€ ê²ƒ\n",
    "# ê³ ìœ ê°’ì´ ì ìœ¼ë©´ ë‹¤ë¥¸ ë³€ìˆ˜ì™€ ê´€ê³„ê°€ ë†’ì„ ìˆ˜ ìˆë‹¤ê³  í•¨\n",
    "\n",
    "for col in cat_columns2:\n",
    "    print(f\"{col}: {concat_select[col].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2465b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(cat_cols)):\n",
    "    for j in range(i + 1, len(cat_cols)):\n",
    "        col1, col2 = cat_cols[i], cat_cols[j]\n",
    "        val = cramer_matrix.loc[col1, col2]\n",
    "        if pd.notnull(val) and float(val) >= threshold:\n",
    "            high_corr_pairs.append((col1, col2, float(val)))\n",
    "\n",
    "for col1, col2, score in sorted(high_corr_pairs, key=lambda x: -x[2]):\n",
    "    print(f\"ğŸ” {col1} â†” {col2} : CramÃ©r's V = {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504902c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ìƒì¹˜ íƒìƒ‰\n",
    "\n",
    "def detect_outliers_iqr(df, columns, iqr_scale=1.5):\n",
    "    outlier_summary = []\n",
    "\n",
    "    for col in columns:\n",
    "        if df[col].isnull().all():\n",
    "            continue\n",
    "\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - iqr_scale * IQR\n",
    "        upper_bound = Q3 + iqr_scale * IQR\n",
    "\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_count = outliers.shape[0]\n",
    "        outlier_ratio = outlier_count / df.shape[0] * 100\n",
    "\n",
    "        outlier_summary.append({\n",
    "            'ë³€ìˆ˜': col,\n",
    "            'ì´ìƒì¹˜ ê°œìˆ˜': outlier_count,\n",
    "            'ì´ìƒì¹˜ ë¹„ìœ¨(%)': round(outlier_ratio, 2)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(outlier_summary).sort_values('ì´ìƒì¹˜ ë¹„ìœ¨(%)', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ìƒì¹˜ ë¹„ìœ¨ ìƒìœ„ 5ê°œ ë³€ìˆ˜ ì¶”ì¶œ\n",
    "top_outlier_cols = outlier_df['ë³€ìˆ˜'].head(5)\n",
    "\n",
    "# Boxplot ì‹œê°í™”\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, col in enumerate(top_outlier_cols, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.boxplot(x=train_data[col])\n",
    "    plt.title(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
