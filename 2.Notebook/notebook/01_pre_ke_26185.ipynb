{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9059cc5",
   "metadata": {},
   "source": [
    "# ì „ì²˜ë¦¬ ê³¼ì •\n",
    "\n",
    "- RMSE : 26185.4795\n",
    "- ë°ì´í„° : train_csv, test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35418d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ í›„\n",
    "# ì „ì²˜ë¦¬ ìœ„í•œ ë°ì´í„°ì…‹ í•©ì¹˜ê¸°\n",
    "\n",
    "train['data'] = 0\n",
    "test['data'] = 1\n",
    "concat = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee0d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë¦„ ë°”ê¾¸ê¸°\n",
    "\n",
    "concat = concat.rename(columns={'ì „ìš©ë©´ì (ã¡)':'ì „ìš©ë©´ì '})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27417357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³¸ë²ˆ, ë¶€ë²ˆì˜ ê²½ìš° floatë¡œ ë˜ì–´ìˆì§€ë§Œ ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì˜ë¯¸ë¥¼ ê°€ì§€ë¯€ë¡œ object(string) í˜•íƒœë¡œ ë°”ê¾¸ê¸°\n",
    "concat_select['ë³¸ë²ˆ'] = concat_select['ë³¸ë²ˆ'].astype('str')\n",
    "concat_select['ë¶€ë²ˆ'] = concat_select['ë¶€ë²ˆ'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f8105",
   "metadata": {},
   "source": [
    "### ê²°ì¸¡ì¹˜ íƒìƒ‰ ë° ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ íƒìƒ‰ í›„ ê²°ì¸¡ë¥  ë³´ê¸°\n",
    "# ì´í›„ì— ì˜ë¯¸ ì—†ëŠ” ê°’ë“¤ ì°¾ì•„ì„œ ì±„ìš°ê¸°\n",
    "# ìœ„ ì²˜ëŸ¼ ì•„ë¬´ ì˜ë¯¸ë„ ê°–ì§€ ì•ŠëŠ” ì¹¼ëŸ¼ì€ ê²°ì¸¡ì¹˜ì™€ ê°™ì€ ì—­í• ì„ í•˜ë¯€ë¡œ, np.nanìœ¼ë¡œ ì±„ì›Œ ê²°ì¸¡ì¹˜ë¡œ ì¸ì‹ì‹œí‚´\n",
    "\n",
    "concat['ë„ë¡œëª…'] = concat['ë„ë¡œëª…'].replace(' ', np.nan)\n",
    "concat['ë“±ê¸°ì‹ ì²­ì¼ì'] = concat['ë“±ê¸°ì‹ ì²­ì¼ì'].replace(' ', np.nan)\n",
    "concat['ê±°ë˜ìœ í˜•'] = concat['ê±°ë˜ìœ í˜•'].replace('-', np.nan)\n",
    "concat['ì¤‘ê°œì‚¬ì†Œì¬ì§€'] = concat['ì¤‘ê°œì‚¬ì†Œì¬ì§€'].replace('-', np.nan)\n",
    "concat['k-ì‹œí–‰ì‚¬'] = concat['k-ì‹œí–‰ì‚¬'].replace('.', np.nan)\n",
    "concat['k-ì‹œí–‰ì‚¬'] = concat['k-ì‹œí–‰ì‚¬'].replace('-', np.nan)\n",
    "concat['k-í™ˆí˜ì´ì§€'] = concat['k-í™ˆí˜ì´ì§€'].replace('ì—†ìŒ', np.nan)\n",
    "concat['k-í™ˆí˜ì´ì§€'] = concat['k-í™ˆí˜ì´ì§€'].replace('.', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f07d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(concat.shape[0] * 0.8) = 902475.2000000001\n",
    "# ê²°ì¸¡ê°’ì´ 80%ì´ìƒì¸(Nullê°’ì´ 90ë§Œê°œ ì´ìƒ) ì¹¼ëŸ¼ì€ ì‚­ì œ\n",
    "print('* ê²°ì¸¡ì¹˜ê°€ 90ë§Œê°œ ì´í•˜ì¸ ë³€ìˆ˜ë“¤ :', list(concat.columns[concat.isnull().sum() <= 900000]))     # ë‚¨ê²¨ì§ˆ ë³€ìˆ˜ë“¤ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "print('* ê²°ì¸¡ì¹˜ê°€ 90ë§Œê°œ ì´ìƒì¸ ë³€ìˆ˜ë“¤ :', list(concat.columns[concat.isnull().sum() >= 900000]))\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ 90ë§Œê°œ ì´ìƒì¸ ê°’ê³¼ ì´í•˜ì§€ë§Œ í•„ìš”ì—†ëŠ” ê²ƒ ì œì™¸\n",
    "# í•„ìš”ì—†ì–´ ë³´ì´ëŠ” ê²ƒ : k-ì „í™”ë²ˆí˜¸, k-íŒ©ìŠ¤ë²ˆí˜¸, ì‚¬ìš©í—ˆê°€ì—¬ë¶€, ê´€ë¦¬ë¹„ ì—…ë¡œë“œ, k-ìˆ˜ì •ì¼ì\n",
    "valid_cols = concat.columns[concat.isnull().sum() <= 900000]\n",
    "exclude_cols = ['k-ì „í™”ë²ˆí˜¸', 'k-íŒ©ìŠ¤ë²ˆí˜¸', 'ì‚¬ìš©í—ˆê°€ì—¬ë¶€', 'ê´€ë¦¬ë¹„ ì—…ë¡œë“œ', 'k-ìˆ˜ì •ì¼ì']\n",
    "\n",
    "select = [col for col in valid_cols if col not in exclude_cols]\n",
    "concat_select = concat[select]\n",
    "\n",
    "concat.shape, concat_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¨¼ì €, ì—°ì†í˜• ë³€ìˆ˜ì™€ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìœ„ infoì— ë”°ë¼ ë¶„ë¦¬\n",
    "# ìˆ«ìí˜• ë¶„ë¦¬ pd.api.types.is_numeric_dtype\n",
    "con_columns = []\n",
    "cat_columns = []\n",
    "\n",
    "for column in concat_select.columns:\n",
    "    if pd.api.types.is_numeric_dtype(concat_select[column]):\n",
    "        con_columns.append(column)\n",
    "    else:\n",
    "        cat_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ee3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ìƒê´€ê´€ê³„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ feature ìŒ íƒì§€ ë° ì‚­ì œ í›„ë³´ ì¶”ì²œ\n",
    "\n",
    "# ì—°ì†í˜• ë³€ìˆ˜ë§Œ ì¶”ì¶œ\n",
    "numeric_cols = con_columns  # â† ë„ˆê°€ ë‚˜ëˆ ë‘” ì—°ì†í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# ìƒê´€ê´€ê³„ í–‰ë ¬ (ì ˆëŒ“ê°’ ê¸°ì¤€)\n",
    "corr_matrix = concat_select[numeric_cols].corr().abs()\n",
    "\n",
    "# ìƒì‚¼ê° í–‰ë ¬ë¡œ ì¤‘ë³µ ì œê±°\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# ìƒê´€ê³„ìˆ˜ 0.9 ì´ˆê³¼ì¸ ë³€ìˆ˜ìŒ ì¶”ì¶œ\n",
    "high_corr_pairs = [(col, row, upper.loc[row, col])\n",
    "                   for col in upper.columns\n",
    "                   for row in upper.index\n",
    "                   if pd.notnull(upper.loc[row, col]) and upper.loc[row, col] > 0.7]\n",
    "\n",
    "# ì¶œë ¥\n",
    "for col1, col2, score in sorted(high_corr_pairs, key=lambda x: -x[2]):\n",
    "    print(f\"ğŸ” {col1} â†” {col2} : ìƒê´€ê³„ìˆ˜ = {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™© pca\n",
    "\n",
    "pca_cols = [\n",
    "    'k-ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡ì´í•˜)',\n",
    "    'k-ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡~85ã¡ì´í•˜)',\n",
    "    'k-85ã¡~135ã¡ì´í•˜']\n",
    "pca_data = concat_select[pca_cols].fillna(0)  # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ê²°ì¸¡ 0ìœ¼ë¡œ ëŒ€ì²´\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_pca_data = scaler.fit_transform(pca_data)\n",
    "\n",
    "# 2ê°œ ì„±ë¶„ìœ¼ë¡œ ì¶•ì†Œ\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)  \n",
    "pca_components = pca.fit_transform(scaled_pca_data)\n",
    "\n",
    "# PCA ì„¤ëª…ë ¥ ë³´ê¸°\n",
    "print(pca.explained_variance_ratio_)  # ì˜ˆ: [0.83, 0.16]\n",
    "\n",
    "# PCA ê²°ê³¼ ì €ì¥\n",
    "concat_select[\"ì„¸ëŒ€ë©´ì _PCA1\"] = pca_components[:, 0]\n",
    "concat_select[\"ì„¸ëŒ€ë©´ì _PCA2\"] = pca_components[:, 1]\n",
    "\n",
    "# ì›ë³¸ feature ì œê±°\n",
    "concat_select.drop(columns=pca_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56054295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—°ì†í˜• ë³€ìˆ˜ ìƒê´€ê´€ê³„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µì˜ ì˜ë¯¸ë¥¼ ê°–ëŠ” ê°’ ì‚­ì œ\n",
    "\n",
    "drop_cols = ['k-ê´€ë¦¬ë¹„ë¶€ê³¼ë©´ì ','k-ì—°ë©´ì ','k-ì „ì²´ë™ìˆ˜']\n",
    "concat_select.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b77647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—°ì†í˜• ë³€ìˆ˜ ë™ ë‹¨ìœ„ í‰ê· ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°\n",
    "# ë™ì´ ì—†ìœ¼ë©´ êµ¬, êµ¬ê°€ ì—†ìœ¼ë©´ ì „ì²´ í‰ê· ìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "# targetì€ ê±´ë“¤ì§€ ë§ì•„ë³´ì\n",
    "\n",
    "concat_select['êµ¬'] = concat_select['ì‹œêµ°êµ¬'].str.split().str[1]\n",
    "concat_select['ë™'] = concat_select['ì‹œêµ°êµ¬'].str.split().str[2]\n",
    "\n",
    "impute_targets = ['ê±´ì¶•ë©´ì ', 'ì£¼ì°¨ëŒ€ìˆ˜', 'ì¢Œí‘œX', 'ì¢Œí‘œY', 'k-ì£¼ê±°ì „ìš©ë©´ì ', 'k-ì „ì²´ì„¸ëŒ€ìˆ˜']\n",
    "\n",
    "for col in impute_targets:\n",
    "    # 1ì°¨: ë™ ë‹¨ìœ„ í‰ê· \n",
    "    concat_select[col] = concat_select.groupby('ë™')[col].transform(lambda x: x.fillna(x.mean()))\n",
    "    # 2ì°¨: êµ¬ ë‹¨ìœ„ í‰ê·  (ë™ í‰ê· ì´ ì•ˆ ë˜ë©´ ì—¬ê¸°ì„œ)\n",
    "    concat_select[col] = concat_select.groupby('êµ¬')[col].transform(lambda x: x.fillna(x.mean()))\n",
    "    # 3ì°¨: ì „ì²´ í‰ê·  (êµ¬ í‰ê· ë„ ì•ˆ ë˜ë©´ ì—¬ê¸°ì„œ)\n",
    "    concat_select[col].fillna(concat_select[col].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8346878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²”ì£¼í˜• featureë“¤ ê´€ê³„ ë³´ê¸°\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# CramÃ©r's V ê³„ì‚° í•¨ìˆ˜\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2 / min(k - 1, r - 1))\n",
    "\n",
    "# ë²”ì£¼í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "cat_cols = cat_columns2  # ì´ë¯¸ ë‚˜ëˆˆ ë¦¬ìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í¬ë˜ë¨¸ìŠ¤ ë¸Œì´ ê¸°ì¤€ ê´€ê³„ìˆëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ ì‚­ì œ\n",
    "drop_cat_cols = [\n",
    "    'ë³¸ë²ˆ',\n",
    "    'ë¶€ë²ˆ',\n",
    "    'ë„ë¡œëª…',\n",
    "    'ë‹¨ì§€ìŠ¹ì¸ì¼',\n",
    "    'ë‹¨ì§€ì‹ ì²­ì¼',\n",
    "    'k-ì„¸ëŒ€íƒ€ì…(ë¶„ì–‘í˜•íƒœ)',\n",
    "    'k-ê´€ë¦¬ë°©ì‹',\n",
    "    'k-ë‚œë°©ë°©ì‹',\n",
    "    'k-ë³µë„ìœ í˜•',\n",
    "    'ì„¸ëŒ€ì „ê¸°ê³„ì•½ë°©ë²•',\n",
    "    'ê²½ë¹„ë¹„ê´€ë¦¬í˜•íƒœ',\n",
    "    'ì²­ì†Œë¹„ê´€ë¦¬í˜•íƒœ',\n",
    "    'ê¸°íƒ€/ì˜ë¬´/ì„ëŒ€/ì„ì˜=1/2/3/4',\n",
    "]\n",
    "\n",
    "# ì œê±° ì ìš©\n",
    "concat_select.drop(columns=drop_cat_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391fc447",
   "metadata": {},
   "source": [
    "### ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30144e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQRë¡œ ì´ìƒì¹˜ íƒì§€\n",
    "def detect_outliers_iqr(df, columns, iqr_scale=1.5):\n",
    "    outlier_summary = []\n",
    "\n",
    "    for col in columns:\n",
    "        if df[col].isnull().all():\n",
    "            continue\n",
    "\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - iqr_scale * IQR\n",
    "        upper_bound = Q3 + iqr_scale * IQR\n",
    "\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_count = outliers.shape[0]\n",
    "        outlier_ratio = outlier_count / df.shape[0] * 100\n",
    "\n",
    "        outlier_summary.append({\n",
    "            'ë³€ìˆ˜': col,\n",
    "            'ì´ìƒì¹˜ ê°œìˆ˜': outlier_count,\n",
    "            'ì´ìƒì¹˜ ë¹„ìœ¨(%)': round(outlier_ratio, 2)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(outlier_summary).sort_values('ì´ìƒì¹˜ ë¹„ìœ¨(%)', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6499d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca ì„¤ëª…ë ¥ì´ ë†’ì§€ë„ ì•Šê³  ì´ìƒì¹˜ë„ ë§ì•„ì„œ pca ì œê±°\n",
    "# ì›ë³¸ k-ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡~85ã¡ì´í•˜) ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "# 1. PCAë¡œ ë§Œë“  feature ì œê±°\n",
    "pca_cols = ['ì„¸ëŒ€ë©´ì _PCA1', 'ì„¸ëŒ€ë©´ì _PCA2']\n",
    "concat_select.drop(columns=pca_cols, inplace=True, errors='ignore')\n",
    "\n",
    "# 2. ì›ë³¸ì—ì„œ íŠ¹ì • ë³€ìˆ˜ë§Œ ê°€ì ¸ì™€ì„œ ì¶”ê°€\n",
    "selected_feature = 'k-ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡~85ã¡ì´í•˜)'\n",
    "concat_select[selected_feature] = concat[selected_feature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_selectê°€ ì•„ë‹Œ concatì—ì„œ ê°€ì ¸ì™€ì„œ ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "\n",
    "concat_select['k-ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡~85ã¡ì´í•˜)'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f076419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ëŠ” ë™/êµ¬/ì „ì²´ í‰ê· ìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "\n",
    "impute2_targets = ['k-ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡~85ã¡ì´í•˜)']\n",
    "\n",
    "for col in impute2_targets:\n",
    "    # 1ì°¨: ë™ ë‹¨ìœ„ í‰ê· \n",
    "    concat_select[col] = concat_select.groupby('ë™')[col].transform(lambda x: x.fillna(x.mean()))\n",
    "    # 2ì°¨: êµ¬ ë‹¨ìœ„ í‰ê·  (ë™ í‰ê· ì´ ì•ˆ ë˜ë©´ ì—¬ê¸°ì„œ)\n",
    "    concat_select[col] = concat_select.groupby('êµ¬')[col].transform(lambda x: x.fillna(x.mean()))\n",
    "    # 3ì°¨: ì „ì²´ í‰ê·  (êµ¬ í‰ê· ë„ ì•ˆ ë˜ë©´ ì—¬ê¸°ì„œ)\n",
    "    concat_select[col].fillna(concat_select[col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f63764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ ì ìš© ë° ë‹¤ìŒ ì´ìƒì¹˜ íƒì§€\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "con_columns_final = []\n",
    "cat_columns_final = []\n",
    "\n",
    "# concat_select ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬\n",
    "for col in concat_select.columns:\n",
    "    if pd.api.types.is_numeric_dtype(concat_select[col]):\n",
    "        con_columns_final.append(col)\n",
    "    else:\n",
    "        cat_columns_final.append(col)\n",
    "\n",
    "def detect_outliers_iqr(df, columns, iqr_scale=1.5):\n",
    "    outlier_summary = []\n",
    "\n",
    "    for col in columns:\n",
    "        if df[col].isnull().all():\n",
    "            continue\n",
    "\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - iqr_scale * IQR\n",
    "        upper_bound = Q3 + iqr_scale * IQR\n",
    "\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_count = outliers.shape[0]\n",
    "        outlier_ratio = outlier_count / df.shape[0] * 100\n",
    "\n",
    "        outlier_summary.append({\n",
    "            'ë³€ìˆ˜': col,\n",
    "            'ì´ìƒì¹˜ ê°œìˆ˜': outlier_count,\n",
    "            'ì´ìƒì¹˜ ë¹„ìœ¨(%)': round(outlier_ratio, 2)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(outlier_summary).sort_values('ì´ìƒì¹˜ ë¹„ìœ¨(%)', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ìš©ì„¸ëŒ€ë³„ë©´ì PCA ë³€ìˆ˜ ì™¸ ì´ìƒì¹˜ ë°œê²¬ê°’ clip\n",
    "\n",
    "def clip_iqr(df, columns, k=1.5):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - k * IQR\n",
    "        upper = Q3 + k * IQR\n",
    "        df[col] = df[col].clip(lower, upper)\n",
    "    return df\n",
    "\n",
    "clip_cols = ['ê±´ì¶•ë©´ì ', 'ì „ìš©ë©´ì ', 'k-ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡~85ã¡ì´í•˜)', 'k-ì „ì²´ì„¸ëŒ€ìˆ˜', 'ì£¼ì°¨ëŒ€ìˆ˜']\n",
    "\n",
    "concat_select = clip_iqr(concat_select, clip_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01b804b",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ea07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ì œ ë‹¤ì‹œ trainê³¼ test datasetì„ ë¶„í• í•´ì¤ë‹ˆë‹¤. ìœ„ì—ì„œ ì œì‘í•´ ë†“ì•˜ë˜ is_test ì¹¼ëŸ¼ì„ ì´ìš©í•©ë‹ˆë‹¤.\n",
    "dt_train = concat_select.query('data==0')\n",
    "dt_test = concat_select.query('data==1')\n",
    "\n",
    "# ì´ì œ is_test ì¹¼ëŸ¼ì€ dropí•´ì¤ë‹ˆë‹¤.\n",
    "dt_train.drop(['data'], axis = 1, inplace=True)\n",
    "dt_test.drop(['data'], axis = 1, inplace=True)\n",
    "print(dt_train.shape, dt_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eafc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©\n",
    "# íŒŒìƒë³€ìˆ˜ ì œì‘ìœ¼ë¡œ ì¶”ê°€ëœ ë³€ìˆ˜ë“¤ì´ ì¡´ì¬í•˜ê¸°ì—, ë‹¤ì‹œí•œë²ˆ ì—°ì†í˜•ê³¼ ë²”ì£¼í˜• ì¹¼ëŸ¼ì„ ë¶„ë¦¬í•´ì£¼ê² ìŠµë‹ˆë‹¤.\n",
    "continuous_columns_final = []\n",
    "categorical_columns_final = []\n",
    "\n",
    "for column in dt_train.columns:\n",
    "    if pd.api.types.is_numeric_dtype(dt_train[column]):\n",
    "        continuous_columns_final.append(column)\n",
    "    else:\n",
    "        categorical_columns_final.append(column)\n",
    "\n",
    "print(\"ì—°ì†í˜• ë³€ìˆ˜:\", continuous_columns_final)\n",
    "print(\"ë²”ì£¼í˜• ë³€ìˆ˜:\", categorical_columns_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f826c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•„ë˜ì—ì„œ ë²”ì£¼í˜• ë³€ìˆ˜ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ ë ˆì´ë¸”ì¸ì½”ë”©ì„ ì§„í–‰í•´ ì£¼ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "# ê° ë³€ìˆ˜ì— ëŒ€í•œ LabelEncoderë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "label_encoders = {}\n",
    "\n",
    "# Implement Label Encoding\n",
    "for col in tqdm( categorical_columns_final ):\n",
    "    lbl = LabelEncoder()\n",
    "\n",
    "    # Label-Encodingì„ fit\n",
    "    lbl.fit( dt_train[col].astype(str) )\n",
    "    dt_train[col] = lbl.transform(dt_train[col].astype(str))\n",
    "    label_encoders[col] = lbl           # ë‚˜ì¤‘ì— í›„ì²˜ë¦¬ë¥¼ ìœ„í•´ ë ˆì´ë¸”ì¸ì½”ë”ë¥¼ ì €ì¥í•´ì£¼ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "    # Test ë°ì´í„°ì—ë§Œ ì¡´ì¬í•˜ëŠ” ìƒˆë¡œ ì¶œí˜„í•œ ë°ì´í„°ë¥¼ ì‹ ê·œ í´ë˜ìŠ¤ë¡œ ì¶”ê°€í•´ì¤ë‹ˆë‹¤.\n",
    "    for label in np.unique(dt_test[col]):\n",
    "      if label not in lbl.classes_: # unseen label ë°ì´í„°ì¸ ê²½ìš°\n",
    "        lbl.classes_ = np.append(lbl.classes_, label) # ë¯¸ì²˜ë¦¬ ì‹œ ValueErrorë°œìƒí•˜ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”!\n",
    "\n",
    "    dt_test[col] = lbl.transform(dt_test[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targetê³¼ ë…ë¦½ë³€ìˆ˜ë“¤ì„ ë¶„ë¦¬í•´ì¤ë‹ˆë‹¤.\n",
    "y_train = dt_train['target']\n",
    "X_train = dt_train.drop(['target'], axis=1)\n",
    "\n",
    "# Hold out splitì„ ì‚¬ìš©í•´ í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ê² ìŠµë‹ˆë‹¤.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=2023)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
