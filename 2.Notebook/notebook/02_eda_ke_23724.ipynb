{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab59052c",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "- RMSE : 23724.4207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c21da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install eli5==0.13.0\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "!apt-get install -y fonts-nanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # ttf íŒŒì¼ì´ ì €ì¥ë˜ì–´ ìˆëŠ” ê²½ë¡œ\n",
    "    name='NanumBarunGothic')                        # ì´ í°íŠ¸ì˜ ì›í•˜ëŠ” ì´ë¦„ ì„¤ì •\n",
    "fm.fontManager.ttflist.insert(0, fe)              # Matplotlibì— í°íŠ¸ ì¶”ê°€\n",
    "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # í°íŠ¸ ì„¤ì •\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "import seaborn as sns\n",
    "\n",
    "# utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d12333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—´ ë‹¤ ë³´ì´ê²Œ ì„¤ì •\n",
    "pd.set_option('display.max_columns', None)\n",
    "# í–‰ ë‹¤ ë³´ì´ê²Œ ì„¤ì •\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636820bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ ìœ„í•´ ë°ì´í„° í•©ì¹˜ê³  í™•ì¸\n",
    "\n",
    "train['data'] = 0\n",
    "test['data'] = 1\n",
    "concat = pd.concat([train, test])\n",
    "\n",
    "print(concat.shape)\n",
    "print(concat['data'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ íƒìƒ‰ ë° ë³´ê°„\n",
    "\n",
    "# ì—´ ì „ì²´ë¥¼ ë„£ê³  ìŠ¤ìº”í•˜ê¸°\n",
    "\n",
    "for col in concat.columns:\n",
    "    nunique = concat[col].nunique(dropna=False)\n",
    "    missing_ratio = concat[col].isna().mean()\n",
    "    missing_count = concat[col].isnull().sum()\n",
    "    col_type = concat.dtypes[col]\n",
    "    print(f\"ğŸ“Œ {col:30} | ë°ì´í„°íƒ€ì…: {col_type} | ê³ ìœ ê°’: {nunique:6} | ê²°ì¸¡ê°œìˆ˜: {missing_count} | ê²°ì¸¡ë¥ : {missing_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ëŠ” ì•„ë‹Œë° ì˜ë¯¸ ì—†ëŠ” í˜•ì‹ì  ê°’ ì°¾ê¸°\n",
    "\n",
    "def detect_fake_nulls(df, suspect_values=['-', ' ', '', '.', 'ì—†ìŒ', 'nan']):\n",
    "    result = {}\n",
    "    for col in df.columns:\n",
    "        if concat[col].dtype == 'object':\n",
    "            val_counts = concat[col].value_counts(dropna=False)\n",
    "            found = val_counts[val_counts.index.isin(suspect_values)]\n",
    "            if not found.empty:\n",
    "                result[col] = found\n",
    "    return result\n",
    "\n",
    "fake_nulls = detect_fake_nulls(concat)\n",
    "for col, vals in fake_nulls.items():\n",
    "    print(f\"ğŸ” {col} ì»¬ëŸ¼ì—ì„œ ì˜ë¯¸ ì—†ëŠ” ê°’ ë°œê²¬:\")\n",
    "    print(vals)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27535b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ë¯¸ì—†ëŠ” ê°’ ê²°ì¸¡ì¹˜ë¡œ ì¸ì‹ì‹œí‚¤ê³  ê²°ì¸¡ë¥  ë³´ê¸°\n",
    "# ì—´ ì „ì²´ë¥¼ ë„£ê³  ìŠ¤ìº”í•˜ê¸°\n",
    "\n",
    "for col in concat.columns:\n",
    "    nunique = concat[col].nunique(dropna=False)\n",
    "    missing_ratio = concat[col].isna().mean()\n",
    "    missing_count = concat[col].isnull().sum()\n",
    "    col_type = concat.dtypes[col]\n",
    "    print(f\"ğŸ“Œ {col:30} | ë°ì´í„°íƒ€ì…: {col_type} | ê³ ìœ ê°’: {nunique:6} | ê²°ì¸¡ê°œìˆ˜: {missing_count} | ê²°ì¸¡ë¥ : {missing_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ ì±„ìš°ê³  info ë³¸ í›„ì— ë°ì´í„° íƒ€ì… ë³€í™˜\n",
    "# ë³¸ë²ˆ, ë¶€ë²ˆì˜ ê²½ìš° floatë¡œ ë˜ì–´ìˆì§€ë§Œ ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì˜ë¯¸ë¥¼ ê°€ì§€ë¯€ë¡œ object(string) í˜•íƒœë¡œ ë°”ê¾¸ì–´ì£¼ê³  ì•„ë˜ ì‘ì—…ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "concat_select['ë³¸ë²ˆ'] = concat_select['ë³¸ë²ˆ'].astype('str')\n",
    "concat_select['ë¶€ë²ˆ'] = concat_select['ë¶€ë²ˆ'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_summary(df, columns):\n",
    "    result = pd.DataFrame({\n",
    "        'ê²°ì¸¡ ê°œìˆ˜': df[columns].isnull().sum(),\n",
    "        'ê²°ì¸¡ ë¹„ìœ¨(%)': df[columns].isnull().mean() * 100\n",
    "    })\n",
    "    return result[result['ê²°ì¸¡ ê°œìˆ˜'] > 0].sort_values('ê²°ì¸¡ ë¹„ìœ¨(%)', ascending=False)\n",
    "\n",
    "print(\"ğŸ“Š ì—°ì†í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ìš”ì•½\")\n",
    "display(null_summary(concat_select, con_columns))\n",
    "\n",
    "print(\"ğŸ“Š ë²”ì£¼í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ìš”ì•½\")\n",
    "display(null_summary(concat_select, cat_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ íƒì§€ ë° ì²˜ë¦¬ ì „ ì—°ì†í˜• ë³€ìˆ˜ ìƒê´€ê´€ê³„ ë³´ê¸°\n",
    "\n",
    "# 1. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ì„ íƒ\n",
    "numeric_cols = concat_select.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# 2. ìƒê´€ê´€ê³„ ê³„ì‚°\n",
    "corr = concat_select[numeric_cols].corr()\n",
    "\n",
    "# 3. ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "plt.title(\"ğŸ“Š ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f63653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒê´€ê´€ê³„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ feature ìŒ íƒì§€ ë° ì‚­ì œ í›„ë³´ ì¶”ì²œ\n",
    "\n",
    "# ì—°ì†í˜• ë³€ìˆ˜ë§Œ ì¶”ì¶œ\n",
    "numeric_cols = con_columns  # â† ë„ˆê°€ ë‚˜ëˆ ë‘” ì—°ì†í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# ìƒê´€ê´€ê³„ í–‰ë ¬ (ì ˆëŒ“ê°’ ê¸°ì¤€)\n",
    "corr_matrix = concat_select[numeric_cols].corr().abs()\n",
    "\n",
    "# ìƒì‚¼ê° í–‰ë ¬ë¡œ ì¤‘ë³µ ì œê±°\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# ìƒê´€ê³„ìˆ˜ 0.9 ì´ˆê³¼ì¸ ë³€ìˆ˜ìŒ ì¶”ì¶œ\n",
    "high_corr_pairs = [(col, row, upper.loc[row, col])\n",
    "                   for col in upper.columns\n",
    "                   for row in upper.index\n",
    "                   if pd.notnull(upper.loc[row, col]) and upper.loc[row, col] > 0.7]\n",
    "\n",
    "# ì¶œë ¥\n",
    "for col1, col2, score in sorted(high_corr_pairs, key=lambda x: -x[2]):\n",
    "    print(f\"ğŸ” {col1} â†” {col2} : ìƒê´€ê³„ìˆ˜ = {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1442e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒê´€ê´€ê³„ ë³´ê³  pcaì™€ ì œê±° ì§„í–‰ í›„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™” ë° ì§€ì •\n",
    "# ë²”ì£¼í˜• ë³€ìˆ˜ ê´€ê³„ ë³´ê¸° ì „ í™•ì¸\n",
    "# ìˆ«ìí˜• ë¶„ë¦¬ pd.api.types.is_numeric_dtype\n",
    "con_columns2 = []\n",
    "cat_columns2 = []\n",
    "\n",
    "for column in concat_select.columns:\n",
    "    if pd.api.types.is_numeric_dtype(concat_select[column]):\n",
    "        con_columns2.append(column)\n",
    "    else:\n",
    "        cat_columns2.append(column)\n",
    "\n",
    "print(\"ì—°ì†í˜• ë³€ìˆ˜:\", con_columns2)\n",
    "print(\"ë²”ì£¼í˜• ë³€ìˆ˜:\", cat_columns2)\n",
    "\n",
    "def null_summary(df, columns):\n",
    "    result = pd.DataFrame({\n",
    "        'ê²°ì¸¡ ê°œìˆ˜': df[columns].isnull().sum(),\n",
    "        'ê²°ì¸¡ ë¹„ìœ¨(%)': df[columns].isnull().mean() * 100\n",
    "    })\n",
    "    return result[result['ê²°ì¸¡ ê°œìˆ˜'] > 0].sort_values('ê²°ì¸¡ ë¹„ìœ¨(%)', ascending=False)\n",
    "\n",
    "print(\"ğŸ“Š ì—°ì†í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ìš”ì•½\")\n",
    "display(null_summary(concat_select, con_columns2))\n",
    "\n",
    "print(\"ğŸ“Š ë²”ì£¼í˜• ë³€ìˆ˜ ê²°ì¸¡ì¹˜ ìš”ì•½\")\n",
    "display(null_summary(concat_select, cat_columns2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²”ì£¼í˜• featureë“¤ ê´€ê³„ ë³´ê¸°\n",
    "# ë³´ê¸° ì „ ê²°ì¸¡ê°’ì€ ì„ì˜ë¡œ ì±„ì›€\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# CramÃ©r's V ê³„ì‚° í•¨ìˆ˜\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2 / min(k - 1, r - 1))\n",
    "\n",
    "# ë²”ì£¼í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "cat_cols = cat_columns2  # ì´ë¯¸ ë‚˜ëˆˆ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© ë§¤íŠ¸ë¦­ìŠ¤\n",
    "cramer_matrix = pd.DataFrame(index=cat_cols, columns=cat_cols)\n",
    "\n",
    "for col1 in cat_cols:\n",
    "    for col2 in cat_cols:\n",
    "        if col1 == col2:\n",
    "            cramer_matrix.loc[col1, col2] = 1.0\n",
    "        else:\n",
    "            try:\n",
    "                cramer_matrix.loc[col1, col2] = cramers_v(concat_select[col1], concat_select[col2])\n",
    "            except:\n",
    "                cramer_matrix.loc[col1, col2] = np.nan\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cramer_matrix.astype(float), cmap='coolwarm', annot=False)\n",
    "plt.title(\"ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ CramÃ©r's V (ìƒê´€ê´€ê³„ ìœ ì‚¬ë„)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ ë‹ˆí¬ ê°’ì´ ë‚®ìœ¼ë©´ ê°’ì´ ë‹¤ì–‘í•˜ì§€ ì•Šì•„ ìœ ì‚¬ì„±ì´ ë†’ì„ ìˆ˜ ìˆë‹¤ê³  í•¨\n",
    "# ë³€ì£¼í˜• ì¤‘ uniqueê°’ì´ ë‚®ì€ ê²ƒ\n",
    "\n",
    "for col in cat_columns2:\n",
    "    print(f\"{col}: {concat_select[col].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c378b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í¬ë˜ë¨¸ìŠ¤ ë¸Œì´ ìˆ˜ì¹˜ë¡œ ë³´ê¸°\n",
    "\n",
    "threshold = 0.8\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(cat_cols)):\n",
    "    for j in range(i + 1, len(cat_cols)):\n",
    "        col1, col2 = cat_cols[i], cat_cols[j]\n",
    "        val = cramer_matrix.loc[col1, col2]\n",
    "        if pd.notnull(val) and float(val) >= threshold:\n",
    "            high_corr_pairs.append((col1, col2, float(val)))\n",
    "\n",
    "for col1, col2, score in sorted(high_corr_pairs, key=lambda x: -x[2]):\n",
    "    print(f\"ğŸ” {col1} â†” {col2} : CramÃ©r's V = {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45bbc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… í™•ì¸\n",
    "# ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "con_columns_final = []\n",
    "cat_columns_final = []\n",
    "\n",
    "# í˜„ì¬ concat_filtered ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°íƒ€ì… ë¶„ë¦¬\n",
    "for col in concat_select.columns:\n",
    "    if pd.api.types.is_numeric_dtype(concat_select[col]):\n",
    "        con_columns_final.append(col)\n",
    "    else:\n",
    "        cat_columns_final.append(col)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"âœ… ì—°ì†í˜• ë³€ìˆ˜:\", con_columns_final)\n",
    "print(\"âœ… ë²”ì£¼í˜• ë³€ìˆ˜:\", cat_columns_final)\n",
    "print(f\"ğŸ“Š ì´ ì—°ì†í˜• ë³€ìˆ˜ ê°œìˆ˜: {len(con_columns_final)}\")\n",
    "print(f\"ğŸ“ ì´ ë²”ì£¼í˜• ë³€ìˆ˜ ê°œìˆ˜: {len(cat_columns_final)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915f778",
   "metadata": {},
   "source": [
    "### íŒŒìƒë³€ìˆ˜ ìƒì„± í›„ ì‚´í´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4267b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_select[['ì§€í•˜ì² _1000më‚´_ê°œìˆ˜', 'ì§€í•˜ì² _1000më‚´_ì´ë¦„ëª©ë¡', 'ì§€í•˜ì² _ìµœë‹¨ê±°ë¦¬_km','ì§€í•˜ì² _ìµœë‹¨ê±°ë¦¬_ì—­ëª…']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§€í•˜ì² ê³¼ ë²„ìŠ¤ ë°ì´í„°ë¡œ íŒŒìƒë³€ìˆ˜ ìƒì„± í›„ ì „ì²´ ì‚´í”¼ê¸°\n",
    "\n",
    "concat_select.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d857a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "con_columns_final = []\n",
    "cat_columns_final = []\n",
    "\n",
    "# concat_select ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬\n",
    "for col in concat_select.columns:\n",
    "    if pd.api.types.is_numeric_dtype(concat_select[col]):\n",
    "        con_columns_final.append(col)\n",
    "    else:\n",
    "        cat_columns_final.append(col)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"âœ… ì—°ì†í˜• ë³€ìˆ˜:\", con_columns_final)\n",
    "print(\"âœ… ë²”ì£¼í˜• ë³€ìˆ˜:\", cat_columns_final)\n",
    "print(f\"ğŸ“Š ì´ ì—°ì†í˜• ë³€ìˆ˜ ê°œìˆ˜: {len(con_columns_final)}\")\n",
    "print(f\"ğŸ“ ì´ ë²”ì£¼í˜• ë³€ìˆ˜ ê°œìˆ˜: {len(cat_columns_final)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ê°€í•œ ë³€ìˆ˜ë“¤ ìƒê´€ê´€ê³„ ë³´ê¸°\n",
    "\n",
    "correlation_with_target = concat_select[con_columns_final].corr()['target'].sort_values(ascending=False)\n",
    "print(correlation_with_target)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(concat_select[con_columns_final].corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"ì—°ì†í˜• ë³€ìˆ˜ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec87464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ê°€í•œ ë³€ìˆ˜(ë²”ì£¼í˜•) ê´€ê³„ ë³´ê¸°\n",
    "\n",
    "# ë²”ì£¼í˜• featureë“¤ ê´€ê³„ ë³´ê¸°\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# CramÃ©r's V ê³„ì‚° í•¨ìˆ˜\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2 / min(k - 1, r - 1))\n",
    "\n",
    "# ë²”ì£¼í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "cat_cols = cat_columns2 = cat_columns_final  # ì´ë¯¸ ë‚˜ëˆˆ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© ë§¤íŠ¸ë¦­ìŠ¤\n",
    "cramer_matrix = pd.DataFrame(index=cat_cols, columns=cat_cols)\n",
    "\n",
    "for col1 in cat_cols:\n",
    "    for col2 in cat_cols:\n",
    "        if col1 == col2:\n",
    "            cramer_matrix.loc[col1, col2] = 1.0\n",
    "        else:\n",
    "            try:\n",
    "                cramer_matrix.loc[col1, col2] = cramers_v(concat_select[col1], concat_select[col2])\n",
    "            except:\n",
    "                cramer_matrix.loc[col1, col2] = np.nan\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cramer_matrix.astype(float), cmap='coolwarm', annot=False)\n",
    "plt.title(\"ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ CramÃ©r's V (ìƒê´€ê´€ê³„ ìœ ì‚¬ë„)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48884b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(cat_cols)):\n",
    "    for j in range(i + 1, len(cat_cols)):\n",
    "        col1, col2 = cat_cols[i], cat_cols[j]\n",
    "        val = cramer_matrix.loc[col1, col2]\n",
    "        if pd.notnull(val) and float(val) >= threshold:\n",
    "            high_corr_pairs.append((col1, col2, float(val)))\n",
    "\n",
    "for col1, col2, score in sorted(high_corr_pairs, key=lambda x: -x[2]):\n",
    "    print(f\"ğŸ” {col1} â†” {col2} : CramÃ©r's V = {score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
